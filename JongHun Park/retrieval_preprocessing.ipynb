{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1b553b92-0d33-4497-bf85-a702bd2dffa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # download elasticsearch\n",
    "# ! wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-7.6.2-linux-x86_64.tar.gz -q\n",
    "# ! tar -xzf elasticsearch-7.6.2-linux-x86_64.tar.gz\n",
    "# ! chown -R daemon:daemon elasticsearch-7.6.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d842772e-9c57-48bb-994f-9c9af40a8055",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # collapse-hide\n",
    "# !pip install elasticsearch\n",
    "# !pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "29c07bb2-0f08-421b-be6b-97fda0d89810",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !elasticsearch-7.6.2/bin/elasticsearch-plugin install analysis-nori"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6d8e2a0-3ff4-4279-97c1-9f7e7649b38c",
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-ab21251a2fbf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkonlpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtag\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mMecab\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import torch\n",
    "import pickle\n",
    "\n",
    "from konlpy.tag import Mecab\n",
    "from tqdm.notebook import tqdm\n",
    "from datasets import load_from_disk\n",
    "from transformers import AutoTokenizer\n",
    "from elasticsearch import Elasticsearch\n",
    "from subprocess import Popen, PIPE, STDOUT\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from datasets import load_metric, load_from_disk, load_dataset, Features, Value, Sequence, DatasetDict, Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2eacdcfd-8cd3-47d2-b89f-449f1e822174",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = load_from_disk(\"/opt/ml/input/data/data/train_dataset\")[\"train\"]\n",
    "validation_file = load_from_disk(\"/opt/ml/input/data/data/train_dataset\")[\"validation\"]\n",
    "test_file = load_from_disk(\"/opt/ml/input/data/data/test_dataset\")[\"validation\"]\n",
    "\n",
    "with open(\"/opt/ml/new_dataset/preprocess_wiki.json\", \"r\") as f:\n",
    "    wiki = json.load(f)\n",
    "\n",
    "wiki_contexts = list(dict.fromkeys([v['text'] for v in wiki.values()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48137bed-9e6f-48fb-a976-c0e37d801cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_records = [{\"example_id\" : train_file[i][\"id\"],\n",
    "               \"document_title\" : train_file[i][\"title\"],\n",
    "               \"question_text\" : train_file[i][\"question\"],\n",
    "               \"answer\" : train_file[i][\"answers\"]} for i in range(len(train_file))]\n",
    "\n",
    "wiki_articles = [{\"document_text\" : wiki_contexts[i]} for i in range(len(wiki_contexts))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bb2e0e95-9591-47f2-afaf-138f9e844fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "es_server = Popen(['elasticsearch-7.6.2/bin/elasticsearch'],\n",
    "                   stdout=PIPE, stderr=STDOUT,\n",
    "                   preexec_fn=lambda: os.setuid(1)  # as daemon\n",
    "                  )\n",
    "# wait until ES has started\n",
    "! sleep 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1b6b2231-0acd-4af2-99ff-37e04a106d12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = {'host':'localhost', 'port':9200}\n",
    "es = Elasticsearch([config])\n",
    "\n",
    "# test connection\n",
    "es.ping()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "58a0ab19-7941-4e8f-8dff-27f0663da81b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'acknowledged': True, 'shards_acknowledged': True, 'index': 'nori-index'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "es.indices.delete(index='nori-index', ignore=[400, 404])\n",
    "index_config = {\n",
    "        \"settings\": {\n",
    "            \"analysis\": {\n",
    "                \"analyzer\": {\n",
    "                    \"nori_analyzer\": {\n",
    "                        \"type\": \"custom\",\n",
    "                        \"tokenizer\": \"nori_tokenizer\",\n",
    "                        \"decompound_mode\": \"mixed\",\n",
    "                        \"stopwords\": \"_korean_\",\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        \"mappings\": {\n",
    "            \"dynamic\": \"strict\", \n",
    "            \"properties\": {\n",
    "                \"document_text\": {\"type\": \"text\", \"analyzer\": \"nori_analyzer\"}\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "\n",
    "index_name = 'nori-index'\n",
    "es.indices.create(index=index_name, body=index_config, ignore=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d054c687-b2c3-4614-9dfc-663c989bc36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def populate_index(es_obj, index_name, evidence_corpus):\n",
    "    '''\n",
    "    Loads records into an existing Elasticsearch index\n",
    "\n",
    "    Args:\n",
    "        es_obj (elasticsearch.client.Elasticsearch) - Elasticsearch client object\n",
    "        index_name (str) - Name of index\n",
    "        evidence_corpus (list) - List of dicts containing data records\n",
    "\n",
    "    '''\n",
    "\n",
    "    for i, rec in enumerate(tqdm(evidence_corpus)):\n",
    "        try:\n",
    "            index_status = es_obj.index(index=index_name, id=i, body=rec)\n",
    "        except:\n",
    "            print(f'Unable to load document {i}.')\n",
    "            \n",
    "    n_records = es_obj.count(index=index_name)['count']\n",
    "    print(f'Succesfully loaded {n_records} into {index_name}')\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2f7f460b-ed24-4b64-8eba-fba9f19d64a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af7b93632f9b44528b96d0b80c115c21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=56606.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Succesfully loaded 56606 into nori-index\n"
     ]
    }
   ],
   "source": [
    "all_wiki_articles = wiki_articles\n",
    "populate_index(es_obj=es, index_name='nori-index', evidence_corpus=all_wiki_articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "76d82acb-c4c7-49b7-b3ba-c5a9084b2de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# collapse-hide\n",
    "def search_es(es_obj, index_name, question_text, n_results):\n",
    "    '''\n",
    "    Execute an Elasticsearch query on a specified index\n",
    "    \n",
    "    Args:\n",
    "        es_obj (elasticsearch.client.Elasticsearch) - Elasticsearch client object\n",
    "        index_name (str) - Name of index to query\n",
    "        query (dict) - Query DSL\n",
    "        n_results (int) - Number of results to return\n",
    "        \n",
    "    Returns\n",
    "        res - Elasticsearch response object\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # construct query\n",
    "    query = {\n",
    "            'query': {\n",
    "                'match': {\n",
    "                    'document_text': question_text\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "    \n",
    "    res = es_obj.search(index=index_name, body=query, size=n_results)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4c39e728-3754-45ac-9e8f-d178fccf809f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mecab = Mecab()\n",
    "def morphs_split(text):\n",
    "    text = mecab.morphs(text)\n",
    "    return ' '.join(text)\n",
    "\n",
    "def preprocess(text):\n",
    "    text = re.sub(r'\\n', ' ', text)\n",
    "    text = re.sub(r\"\\\\n\", \" \", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    text = re.sub(r'#', ' ', text)\n",
    "    text = re.sub(r\"[^a-zA-Z0-9가-힣ㄱ-ㅎㅏ-ㅣぁ-ゔァ-ヴー々〆〤一-龥<>()\\s\\.\\?!》《≪≫\\'<>〈〉:‘’%,『』「」＜＞・\\\"-“”∧]\", \"\", text)\n",
    "    return text\n",
    "\n",
    "def context_split(text):\n",
    "    text = ' '.join(text.strip().split('\\\\n')).strip()\n",
    "    sent_list = text.strip().split('. ')\n",
    "    text = ''\n",
    "    for sent in sent_list:\n",
    "        sent = preprocess(sent)\n",
    "        sent = mecab.morphs(sent)\n",
    "        text += ' '.join(sent)+'[SEP]'\n",
    "    return text[:-5]\n",
    "\n",
    "def sentence_split(text):\n",
    "    text_list = [sent for sent in map(lambda x : x.strip(), text.split('[SEP]')) if sent != '']\n",
    "    return text_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "55c622e9-e867-4f8d-9099-fecf39b29db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train_file = {'context':[], 'id':[], 'question':[], 'top20':[], 'answer_idx':[], 'answer':[], 'start_idx':[]}\n",
    "for file in train_file:\n",
    "    \n",
    "    context = file[\"context\"]\n",
    "    new_context = context_split(context)\n",
    "    new_context =  ' '.join(sentence_split(new_context))\n",
    "    \n",
    "    question_text = file['question']\n",
    "    res = search_es(es_obj=es, index_name='nori-index', question_text=question_text, n_results=20)\n",
    "    top20_list = [hit['_source']['document_text'] for hit in res['hits']['hits']]\n",
    "    \n",
    "    if not new_context in top20_list:\n",
    "        top20_list = top20_list[:-1] + [new_context]\n",
    "        answer_idx = 19\n",
    "    else:\n",
    "        answer_idx = top20_list.index(new_context)\n",
    "    \n",
    "    answer = file['answers']['text'][0]\n",
    "    new_answer = morphs_split(answer)\n",
    "    \n",
    "    start_idx = file[\"answers\"][\"answer_start\"][0] \n",
    "    front_context = file[\"context\"][:start_idx]\n",
    "    back_context = file[\"context\"][start_idx+len(answer):]\n",
    "    \n",
    "    new_front_context = context_split(front_context)\n",
    "    new_back_context = context_split(back_context)\n",
    "    new_front_context =  ' '.join(sentence_split(new_front_context))\n",
    "    new_back_context = ' '.join(sentence_split(new_back_context))\n",
    "    new_context = ' '.join([new_front_context, new_answer, new_back_context])\n",
    "    \n",
    "    ids_move = len(front_context) - len(new_front_context)\n",
    "    start_idx = start_idx - ids_move + 1\n",
    "    \n",
    "    new_question = morphs_split(question_text)\n",
    "    \n",
    "    new_train_file['context'].append(new_context)\n",
    "    new_train_file['id'].append(file['id'])\n",
    "    new_train_file['question'].append(new_question)\n",
    "    new_train_file['top20'].append(top20_list)\n",
    "    new_train_file['answer_idx'].append(answer_idx)\n",
    "    new_train_file['answer'].append(new_answer)\n",
    "    new_train_file['start_idx'].append(start_idx)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a69c014e-1966-4fa2-a37a-60dd86c00f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_valid_file = {'context':[], 'id':[], 'question':[], 'top20':[], 'answer_idx':[], 'answer':[], 'start_idx':[]}\n",
    "for file in validation_file:\n",
    "    \n",
    "    context = file[\"context\"]\n",
    "    new_context = context_split(context)\n",
    "    new_context =  ' '.join(sentence_split(new_context))\n",
    "    \n",
    "    question_text = file['question']\n",
    "    res = search_es(es_obj=es, index_name='nori-index', question_text=question_text, n_results=20)\n",
    "    top20_list = [hit['_source']['document_text'] for hit in res['hits']['hits']]\n",
    "    \n",
    "    if not new_context in top20_list:\n",
    "        top20_list = top20_list[:-1] + [new_context]\n",
    "        answer_idx = 19\n",
    "    else:\n",
    "        answer_idx = top20_list.index(new_context)\n",
    "    \n",
    "    answer = file['answers']['text'][0]\n",
    "    new_answer = morphs_split(answer)\n",
    "    \n",
    "    start_idx = file[\"answers\"][\"answer_start\"][0] \n",
    "    front_context = file[\"context\"][:start_idx]\n",
    "    back_context = file[\"context\"][start_idx+len(answer):]\n",
    "    \n",
    "    new_front_context = context_split(front_context)\n",
    "    new_back_context = context_split(back_context)\n",
    "    new_front_context =  ' '.join(sentence_split(new_front_context))\n",
    "    new_back_context = ' '.join(sentence_split(new_back_context))\n",
    "    new_context = ' '.join([new_front_context, new_answer, new_back_context])\n",
    "    \n",
    "    ids_move = len(front_context) - len(new_front_context)\n",
    "    start_idx = start_idx - ids_move + 1\n",
    "    \n",
    "    new_question = morphs_split(question_text)\n",
    "    \n",
    "    new_valid_file['context'].append(new_context)\n",
    "    new_valid_file['id'].append(file['id'])\n",
    "    new_valid_file['question'].append(new_question)\n",
    "    new_valid_file['top20'].append(top20_list)\n",
    "    new_valid_file['answer_idx'].append(answer_idx)\n",
    "    new_valid_file['answer'].append(new_answer)\n",
    "    new_valid_file['start_idx'].append(start_idx)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f25076f3-c6ff-4575-8b3b-74d84a3772d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_pickle(save_path, data_set):\n",
    "    file = open(save_path, \"wb\")\n",
    "    pickle.dump(data_set, file)\n",
    "    file.close()\n",
    "\n",
    "def get_pickle(pickle_path):\n",
    "    f = open(pickle_path, \"rb\")\n",
    "    dataset = pickle.load(f)\n",
    "    f.close()\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6178ffeb-51bc-4024-a728-14d52b9446c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_pickle(\"/opt/ml/new_dataset/preprocess_train.pkl\", new_train_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c3fef990-cc6f-451d-a664-bc8ef5b15afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_pickle(\"/opt/ml/new_dataset/preprocess_valid.pkl\", new_valid_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80094505-43a8-4e86-91e3-3be26d149519",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python385jvsc74a57bd0337f1a600d345c50cd007a2461b073851b5ec4b77bc6c65adb33d085b42175ad",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}